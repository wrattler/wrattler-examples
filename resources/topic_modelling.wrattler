# TOPIC MODELLING


## Clean and format text data

Firstly, we downloaded PDF files of open source versions of articles and extracted text from the PDFs using [NORMA](https://github.com/ContentMine/norma).
The next step formats this data and prepares it for analysis.






























































































































































































































































































































































































































































```python
import os
os.system("pip3 install nltk")

import nltk
nltk.download('stopwords') 

from nltk.stem.porter import PorterStemmer
from nltk.corpus import stopwords

import re
import pandas as pd

def clean_text(line):
    """
    1. Removes:
    - digits, special characters and punctuation
    - English stop words
    2. Stemms words
    """

    line = line.replace("/", " ").replace("-", " ").replace("  ", " ")
    pattern = re.compile("[^A-Za-z\s]+")
    line = pattern.sub('', line).strip()

    # NOTE: longest word in major dictionary has 45 letters
    stop_words = stopwords.words('english') + ['oxford', 'cambridge', 'warwick', 'edinburgh', 'ucl']
    stemmer = PorterStemmer()
    clean_text = [
        stemmer.stem(word) for word in line.split() 
        if word not in stop_words and len(word) <= 45
        ]

    return " ".join(clean_text)


cleaned_data = pd.DataFrame(columns=['paper_id', 'full_text', 'length'])

for root, _, files in os.walk("resources/data_files/turing-papers/"):
    for file in files:
        if file.endswith(".txt"):
            with open(os.path.join(root, file), 'r') as article:
                cleaned_text = [clean_text(line) for line in article]
                cleaned_data = cleaned_data.append(
                    {"paper_id": root.split('/')[-1], 
                    "full_text": ' '.join(cleaned_text), 
                    "length": len(cleaned_text)
                    }, ignore_index=True
            )

# cleaned_data.to_csv("resources/data_files/cleaned_data.csv", index=False)
```




















Md14: This is a markdown cell.




















```python
import pandas as pd

df = pd.read_csv("resources/data_files/publications_eng.csv", encoding="ISO-8859-1")

df['ak_keywords'] = df.loc[:,'keyword_0':'keyword_26'].fillna('').apply("; ".join, axis=1)
df['paper_id'] = df['paper_id'].apply(str)
df = df.rename(columns={"standard_name":"full_name", "affiliation":"current_uni"})
df = df[['author_id', 'full_name', 'affiliation', 'paper_id', 'ak_keywords']]

full_data = pd.merge(df, cleaned_data, on="paper_id", how="inner")

# full_data.to_csv("resources/data_files/final_dataset_full.csv")
```




























































































































































































































## Import (cleaned) data

Create two dataframes:
- `author_info`: unique ID for each row is a (`name`, `paper_id`) pair where `name` is a Turing researcher
- `publications_data`: each row is a publication with a `paper_id` and the `full_text`

Some articles were published by multiple Turing researchers so `author_info` might have more rows than `publications_data`.
We want to use each article only once when fitting the model and need to keep track of who is associated with each article.




























































































































































































































```python
import pandas as pd

publications_data = pd.read_csv('resources/data_files/final_dataset_full.csv')
publications_data = publications_data.rename(columns={'full_name': 'name', 'current_uni': 'uni'})
publications_data = publications_data.loc[publications_data['name'].isin(
        ['jon crowcroft', 
        'carl rasmussen', 
        'mihaela van der schaar', 
        'ioanna manolopoulou', 
        'theo damoulas'
        ]
    )]

author_info = publications_data.loc[:, publications_data.columns != 'full_text']

publications_data = publications_data.drop_duplicates(subset = 'paper_id')
publications_data = publications_data[
    ['paper_id', 'title', 'ak_keywords', 'full_text']
    ].reset_index(drop=True)
```

































































































































































































































































































































































































































































































































































## Fit model to data

































































































































































































































































































































































































































































































































































```python
import pandas as pd

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

N_TOPICS = 15

vectorizer = CountVectorizer(max_df=0.95, min_df=2)
dtm = vectorizer.fit_transform(publications_data['full_text'].values.astype('U'))
feature_names = vectorizer.get_feature_names()

# fit model
lda = LatentDirichletAllocation(n_components = N_TOPICS,  max_iter = 50,
                            learning_method = 'online', random_state = 0)
lda.fit(dtm)

topic_term_dists = lda.components_ / lda.components_.sum(axis=1)[:, None]
doc_topic_dists = lda.transform(dtm)

#topic_term_dists.to_csv('resources/data_files/topic_term_dists.csv')
#doc_topic_dists.to_csv('resources/data_files/doc_topic_dists.csv')
```


























































## Visualise topics


























































```python
import os
os.system("pip3 install pyLDAvis")

import pyLDAvis
import pyLDAvis.sklearn

prepared_data = pyLDAvis.sklearn.prepare(lda, dtm, vectorizer)
pyLDAvis.display(prepared_data)
```

































































































































































































































































































































































































































## Topic labeling

For now, each topic has a number, it is described as a distribution over the vocabulary and it has associated articles.
For interpretation, we need to assign a label to each topic.

To do that:
1. Get top terms per topic
2. Get top articles per topic (titles and associated keywords)






























































































































































































































































































































































































































```python
import pandas as pd

def _top(df, col):
    return df.sort_values(col, ascending=False).index

def top_terms(topic, n=5):
    return _top(topic_term_dists.T, topic)[0:n]

def top_docs(topic, n=5):
    doc_idx = _top(doc_topic_dists, str(topic))[0:n]
    return publications_data.loc[doc_idx, col]

def topic_data(topic):
    print(top_terms(topic))
    print(top_docs(topic,'title').to_list())
    print(top_docs(topic,'ak_keywords').to_list())

# print top terms, document titles and keywords associated with topic (0-14)
# use this to label topic
for i in range(15):
    print(topic_data(i))
```





























































































































































































































## Top topics by researcher

- For each document, we have a distribution of topics in that document.  
- We also know what researchers are associated with what document.  
- We want to plot the average doc topic distribution for each researcher (as percentages).





























































































































































































































```python
import pandas as pd
import numpy as np

## load results of above analysis 
#doc_topic_dists = pd.read_csv("resources/data_files/doc_topic_dists.csv")

# for each document, combine doc-topic distribution with paper_id
# for each author, get percentage of each topic in their articles
doc_topic_dists.set_index(publications_data['paper_id'], inplace=True)
author_topic_dists = doc_topic_dists.merge(
    author_info[['paper_id', 'name']], 
    left_index=True, 
    right_on='paper_id'
    )
author_topic_dists = author_topic_dists.loc[:, author_topic_dists.columns != 'paper_id'
    ].groupby('name').mean().replace(np.nan, 0)*100

# get overall topic prevalence/importance & combine with above
topic_importance = pd.DataFrame(
    author_topic_dists.sum()/author_topic_dists.shape[0],
    columns = ['topicVal']
    )
topic_data = pd.concat([author_topic_dists.T, topic_importance], axis=1
    ).sort_values(['topicVal'], ascending = 0)

# combine topics with small values into one
other_topic = topic_data.loc[topic_data['topicVal'] < 1.5].sum()
topic_data = topic_data.loc[topic_data['topicVal'] >= 1.5]
topic_data = topic_data.append(other_topic, ignore_index=True)

## save to csv
#topic_data.to_csv("resources/data_files/data_final.csv", index_label='topicNum')
```
