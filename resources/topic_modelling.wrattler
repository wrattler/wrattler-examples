# TOPIC MODELLING


## Import data

Create two dataframes:
- `author_info`: unique ID for each row is a (`name`, `paper_id`) pair where `name` is a Turing researcher
- `publications_data`: each row is a publication with a `paper_id` and the `full_text`

Some articles were published by multiple Turing researchers so `author_info` has more rows than `publications_data`.
We want to use each article only once when fitting our model but then want to attribute it to all related researchers.


```python
import pandas as pd

df = pd.read_csv('resources/data_files/final_dataset_full.csv')
df = df.rename(columns={'full_name': 'name', 'current_uni': 'uni'})

author_info = df.loc[:, df.columns != 'full_text']

publications_data = df.drop_duplicates(subset = 'paper_id')
publications_data = publications_data[['paper_id', 'full_text']].reset_index(drop=True)
```







































































































## Fit model to data







































































































```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

def fit_model(text_data, n_topics = 25):
    """
    Fit LDA model with n_topics to text_data.

    :param text_data: list of texts
    :param n_topics: number of topics to model
    :return: topic-term and document-topic distributions as dataframes
    """
    #create document_term_matrix
    vectorizer = CountVectorizer(max_df=0.95, min_df=2)
    dtm = vectorizer.fit_transform(text_data)
    feature_names = vectorizer.get_feature_names()

    # fit model
    lda = LatentDirichletAllocation(n_components = n_topics,  max_iter = 50,
                                learning_method = 'online', random_state = 0)
    lda.fit(dtm)

    # extract topic-term and document-topic distributions
    topic_term_dists = lda.components_ / lda.components_.sum(axis=1)[:, None]
    doc_topic_dists = lda.transform(dtm)

    return pd.DataFrame(topic_term_dists), pd.DataFrame(doc_topic_dists)

topic_term_dists, doc_topic_dists = fit_model(publications_data['full_text'].values.astype('U'))
```









































## Get data for visualisation










































```python
import pandas as pd

# for each author, get percentage of each topic in their articles
tmp = pd.concat([publications_data['paper_id'], doc_topic_dists], axis=1)
full = tmp.merge(author_info[['paper_id', 'name']], on='paper_id')
author_topic_dists = full.groupby('name'
    )[doc_topic_dists.columns.values].mean().replace(np.nan, 0)*100

# get overall topic prevalence/importance & combine
topic_importance = pd.DataFrame(
    author_topic_dists.sum()/author_topic_dists.shape[0],
    columns = ['topicVal']
    )

topic_data = pd.concat([author_topic_dists.T, topic_importance], axis=1)
topic_data = topic_data.sort_values(['topicVal'], ascending = 0)
```






































































































## Format output















































































































































































## Topic labeling





































































































```python

```
